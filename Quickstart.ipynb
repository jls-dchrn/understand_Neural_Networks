{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1d063c-d2f7-4045-995d-4a41c3c386b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Quickstart tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6f5b73-15c0-41b4-9a2f-20deb44571f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch modules\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e02db3-65bb-431b-91a7-78aabde35a12",
   "metadata": {},
   "source": [
    "# I - Load a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecc4e36e-bae8-42a5-99ac-5efb0de10bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e4b07-7437-432b-8032-ba56e1a774e8",
   "metadata": {},
   "source": [
    " ## FashionMNIST seems to be a dataset about fashion\n",
    " \n",
    " Link for more infos:[here](https://docs.pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST)\n",
    "\n",
    " \n",
    " It is a **dataset of Zalendo's article images**. \n",
    " \n",
    " A training set of 60k examples and a test set of 10k examples.\n",
    " \n",
    " Here is its github: [link](https://github.com/zalandoresearch/fashion-mnist) \n",
    "\n",
    " We can conclude that the \"train\" parameter means that the pytorch dataset **already have separeted training and testing dataset** and we can choose which one to use.\n",
    "\n",
    "\n",
    " download= locally I think\n",
    " \n",
    " transform=ToTensor(), kinda logical: we convert them in torch tensor to process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26fcf870-ecae-4fd6-bfc2-22383b4c1468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d513c-ee07-4971-9fd4-000d0e07b2f8",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "This class is used to load a Pyttorch dataset\n",
    "Only two parameters:\n",
    "- a dataset\n",
    "- its batch size = the number of training example we will give in one forward/backward pass.\n",
    "\n",
    "Increasing the batch size increase the RAM memory needed.\n",
    "\n",
    "Shape NCHW:\n",
    "- N = **number of batchs/ data sample**\n",
    "- C = **number of channels**: for an image, RBG = 3 channels\n",
    "- H = **height**\n",
    "- W = **width**\n",
    "Thoses are the axes in  tensor containing image data sample\n",
    "\n",
    "More infos about Dataloader on the [documentation](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2bd44c-0b42-459e-b44e-e3a5d0cd77b3",
   "metadata": {},
   "source": [
    "# II - Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42bf4475-6796-43b6-b802-3c7c1a2dfd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfd35e8-65a2-4dcf-a1e4-e6aba9bb9d98",
   "metadata": {},
   "source": [
    "To create a neural network in pytorch, we HAVE TO create a class that inherits from **`nn.Module`**.\n",
    "[Here](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html) is the doc for `nn.Module`.\n",
    "\n",
    "We have the choice between using an accelerator such as CUDA or stay on using CPU.\n",
    "\n",
    "[source](https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n",
    "\n",
    "In our class constructor, we first start with `nn.Flatten()`.\n",
    "\n",
    "This class is a layer that convert each 2D 28x28 image into a contiguous array og 784 pixel values.\n",
    "\n",
    "A contiguous array is an array stored in a **unbroken block of memory**, [link](https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays) for illustrated explanations.\n",
    "\n",
    "Then we define a **`nn.Sequential`** attribute called `self.linear_relu_stack`.\n",
    "A Sequential Layer is a container that make our data pass sequentially through multiple layers, here some linear and ReLu layers.\n",
    "[Here](https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) for more info about sequential module.\n",
    "\n",
    "\n",
    "Inside is **`nn.Linear`**. The linear layer is a module that applies a linear transformation (y = ax+b) on the input using its stored weights and biases.\n",
    "\n",
    "Between linear layers, we also use **`nn.ReLu`**. It is a non-linear activation layer detailed [here](https://docs.pytorch.org/docs/stable/generated/torch.nn.ReLU.html) that just does (y=0 if y<=0 else y=x).\n",
    " \n",
    "The constructor `__init__`is called once when the neural network is created. At that time, all layers (the flatten and the sequential) are just defined. They will be called in the **`forward(self,x)`** method.\n",
    "\n",
    "This method is called every time a data is sent through the network.\n",
    "The data `x`is first flattened then passed through the sequential layer (with Relu and Linear layers which mean laers with trainable parameters).\n",
    "\n",
    "The result is returned by this method, it is called logits, an unnormalised output of the model.\n",
    "\n",
    "We ofter normalized them with a softmax function.\n",
    "\n",
    "With those we will be able to define the notion of loss and backpropagation to train our parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c734154-757d-4fb8-8ccf-c8dccdefd733",
   "metadata": {},
   "source": [
    "# III - Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d94ee2c6-928d-467e-a3e1-90781e35e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a7400-2187-46d6-b498-e3f46d95cee4",
   "metadata": {},
   "source": [
    "To train a parameters, we need to define a **loss function** and an **optimizer**.\n",
    "\n",
    "We choose the **cross entropy** loss function between ne numerous differents [loss function available](https://docs.pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "## A quick explaination about the most popular loss functions\n",
    "\n",
    "For each mathematical formula, $y_i$ is the target value, $\\hat{y}_i$ is the predicted value and $N$ is the number of samples.\n",
    "\n",
    "### 1 - Mean Squared Error (MSELoss)\n",
    "\n",
    "This function is used for regression task, when predicting continuous values (temperatures, house prices,...)\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE}(y,\\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\bigl(y_i - \\hat{y}_i\\bigr)^2\n",
    "$$\n",
    "\n",
    "\n",
    "It penalized large error more strongly because we are taking the square of the difference between the target and predicted value.\n",
    "\n",
    "![MSE](https://miro.medium.com/v2/resize:fit:640/format:webp/1*WfVDoLsarrM5HpO9sh_ZQQ.png)\n",
    "\n",
    "### 2 - Mean Absolute Error (L1Loss)\n",
    "\n",
    "It is used in regression tasks where you want robustness to outliers ( valeurs abérantes).\n",
    "\n",
    "$$\n",
    "\\mathrm{MAE}(y,\\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\bigl|y_i - \\hat{y}_i \\bigr|\n",
    "$$\n",
    "\n",
    "It has only a linear penalty and not a square one like in the MSE which make it less sensitive to outliers but it has a less smooth optimization which can lead to sparses gradients.\n",
    "\n",
    "![L1_loss](https://miro.medium.com/v2/resize:fit:640/format:webp/1*0hbNOtpfr6aoR_Bmty-JkA.jpeg)\n",
    "\n",
    "### 3 - Cross Entropy Loss\n",
    "\n",
    "The cross entroy loss is uesd for multi-class classification.\n",
    "\n",
    "We define, for each output (result from each data given to our model), a set of scores for all possibles classes. \n",
    "\n",
    "Then we convert these scores into probabilities that sum to 1 via softmax ( ex : 0,3=30% chance to be a dog, 0,5=50% to be a cat,...)\n",
    "\n",
    "The cross-entropy loss compares those predicted probabilities with the rue class label which is the \"perfect probability distribution: 1 for the correct class and ° on other ( ex:  0% dog, 100% cat)\n",
    "\n",
    "$$\n",
    "\\mathrm{CrossEntropy}(y,\\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log\\left( \\frac{e^{\\hat{y}_{i, y_i}}}{\\sum_{j} e^{\\hat{y}_{i,j}}} \\right)\n",
    "$$\n",
    "\n",
    "#### Explaination:\n",
    "\n",
    "We first apply the **Softmax** function to convert logits into probabilities:\n",
    "$$\n",
    "p_{i,j} = \\frac{e^{\\hat{y}_{i, y_i}}}{\\sum_{j} e^{\\hat{y}_{i,j}}}\n",
    "$$\n",
    "\n",
    "Then the cross-entropy will take the logarithm from each of these probabilities\n",
    "\n",
    "$$\n",
    "\\mathrm{CrossEntropy}(y,\\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log\\left( p_{i,j} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "![cross_entropy](https://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png)\n",
    "\n",
    "### 4 - Binary cross-entropy (BCE)\n",
    "\n",
    "It is used for binary classification ( ex: spam vs not spam)\n",
    "\n",
    "$$\n",
    "\\mathrm{BCE}(y,\\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "The target $y_i$ will always be 0 or 1 but the prediction $\\hat{y}_i \\in [0,1]$ is the probability to be in the class 1 ( ex : class 0 = not spam and class 1 = spam) \n",
    "\n",
    "To obtain this probability we must pass the result of the `nn.Linear` which is a real number (positive or negative) into sigmoid function).\n",
    "\n",
    "Here is the formula of the sigmoid: $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "If we want to proess logits directly we should use `nn.BCEWithLogitsLoss`instead of `nn.BCELoss` which require to apply `torch.sigmoid` on the logits beforehand.\n",
    "\n",
    "![binary_Xentropy](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F54c97fda8af4dccc23d58bd14cd95802df6f1e49-393x272.png&w=640&q=75)\n",
    "\n",
    "### 5 - Negative Log Likelihood Loss (NLLLoss)\n",
    "\n",
    "It is used for multi-class classification when your model already outputs log_probabilities.\n",
    "\n",
    "$$\n",
    "\\mathrm{NLL}(y,\\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p_{i, y_i}\n",
    "$$\n",
    "\n",
    "That's exactly the cross-entropy formula when applied the **LogSoftmax** function (not just the Softmax).\n",
    "\n",
    "In LogSoftmax, instead of computing softmax and then log (which is unstable numerically), PyTorch computes both in one go:\n",
    "$$\n",
    "\\log p_{i,j} = \\hat{y}_{i,j} - \\log  \\left( \\sum_{k=1}^{C} e^{\\hat{y}_i,k} \\right)\n",
    "$$\n",
    "\n",
    "So the full formula is:\n",
    "\n",
    "$$\n",
    "\\mathrm{NLL}(y,\\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\hat{y}_{i,y_i} - \\log  \\left( \\sum_{k=1}^{C} e^{\\hat{y}_i,k} \\right)\n",
    "$$\n",
    "\n",
    "Anyway it is rarely used, **we prefere to use CrossEntropyLoss** directly.\n",
    "\n",
    "### 6 - Huber Loss (`nn.SmoothL1Loss`)\n",
    "\n",
    "Used for regression with both small and lrge error, it is a blend between MSE and MAE.\n",
    "\n",
    "It is less sensitive to outliers than MSE but smoother than MAE.\n",
    "\n",
    "Good general-purpos loss for regression tasks.\n",
    "\n",
    "Because you have to find the $\\delta$ experimentally, you can finetuned to have the best loss possible by finding the best $\\delta$.\n",
    "\n",
    "$$\n",
    "L_{\\delta}(y, \\hat{y}) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{2} (y - \\hat{y})^2, & \\text{if } |y - \\hat{y}| \\le \\delta, \\\\\n",
    "\\delta \\cdot \\bigl(|y - \\hat{y}| - \\tfrac{1}{2}\\delta \\bigr), & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### 7 - KL Divergence Loss (nn.KLDivLoss)\n",
    "\n",
    "Used for comparing probability distribution (ex: in a variational Autoencoder, knowledge distillation)\n",
    "It measures how one probability distribution diverges from another.\n",
    "$$\n",
    "D_{KL}(P||Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7f2b408-963a-4e50-9437-df5d002c191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f651f-be8f-4e90-9900-cb913af34b03",
   "metadata": {},
   "source": [
    "## A quick explaination about the optimizer\n",
    "\n",
    "An optimizer is the lgorithm tha update my model's parameters (weights and biases) based on the computed gradients during training.\n",
    "\n",
    "In a pass we have:\n",
    "- the forward method that generate the logits\n",
    "- the loss that compute the loss\n",
    "- we do a backward pass with the loss (`loss.backward`)\n",
    "- we update the weights with the optimizer (`optimizer.step()` )\n",
    "\n",
    "### 1 - Stochastic Gradient Descent (SGD)\n",
    "\n",
    "This is the **simpliest** and most classic optimizer.\n",
    "\n",
    "**Stochastic** means it use a random subset of data (a batch) instead of the full dataset for each update.\n",
    "If we want to use all data each steps, it is a Batch Gradient Descent, not a SGD.\n",
    "Using one example at a time is called Pure SGD. In practice we use small batch (like 64 samples) per step, it's called Mini-batch SGD.\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\, \\nabla_\\theta L(\\theta_t)\n",
    "$$\n",
    "Where $\\theta_t$ are the parameters (weight) at step t, $\\eta$ is the learning rate and $\\nabla_\\theta L(\\theta_t)$ is the gradient of the loss with respect to the parameters. Here is its formula:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta L(\\theta_t) = \n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial L}{\\partial \\theta_1} \\\\\n",
    "\\dfrac{\\partial L}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{\\partial L}{\\partial \\theta_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "When using `torch.optim.SGD()`the first parameter given are the model parameters (`model.parameters()`) and we also define the learning rate `lr`).\n",
    "\n",
    "The learning rate controls how big each update step is. We have to find the best learning rate experimentally.\n",
    "\n",
    "For instance, `lr = 1e-3` make some small, gentle updates whereas `lr= 1e-1` make large agressive updates.\n",
    "\n",
    "The limitations of this forumla is:\n",
    "- it has the same learning rate $\\eta$ for all parameters\n",
    "- it is sensitive to scale of gradients\n",
    "- it can oscillate and converge slowly, especially in deep networks.\n",
    "\n",
    "-> this optimizer is good for small or simple models\n",
    "\n",
    "### 2 - SGD with momentum\n",
    "\n",
    "We can enhance the SGD rule by adding a \"momentum\" to make training faster and smoother.\n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) \\, C\n",
    "$$\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta v_t\n",
    "$$\n",
    "\n",
    "This velocity term `v_t`accumulate previous gradients, it helps to reduce oscillation by accelerating in consistent gradient direction.\n",
    "\n",
    "We usually define the momentum $\\beta = 0.9$ \n",
    "\n",
    "-> this optimizer is commonly used in CNNs\n",
    "\n",
    "### 3 - Adagrad\n",
    "\n",
    "$$\n",
    "G_t = G_{t-1} + (\\nabla_\\theta L(\\theta_t))^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\varepsilon}} \\, \\nabla_\\theta L(\\theta_t)\n",
    "$$\n",
    "\n",
    "\n",
    "-> this optimizer is used in sparse data or in Natural Language Processing (NPL)\n",
    "\n",
    "\n",
    "### 4 - RMSProp\n",
    "\n",
    "$$\n",
    "E[g^2]_t = \\alpha E[g^2]_{t-1} + (1 - \\alpha) \\, (\\nabla_\\theta L(\\theta_t))^2 \\\\\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\varepsilon}} \\, \\nabla_\\theta L(\\theta_t)\n",
    "$$\n",
    "\n",
    "\n",
    "### 5 - Adaptive Moment Estimation (Adam)\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\, \\nabla_\\theta L(\\theta_t) \\\\\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\, (\\nabla_\\theta L(\\theta_t))^2 \\\\\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\\\\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}\n",
    "$$\n",
    "\n",
    "-> this optimizer is default for most models\n",
    "\n",
    "\n",
    "### 6 - AdamW (Adam with Decoupled Weight Decay)\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\left( \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon} + \\lambda \\theta_t \\right)\n",
    "$$\n",
    "\n",
    "-> we prefer to use this model from Transformers, LLMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c623a19e-ef1d-494a-8933-ddbde5996dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
