{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48448c2-6258-4586-b80c-22fc23630c44",
   "metadata": {},
   "source": [
    "# Pytorch Quickstart tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466bd91b-9dd7-4cf5-a117-d605b4e4effc",
   "metadata": {},
   "source": [
    "[Here](https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) is the source quickstart pytorch tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6f5b73-15c0-41b4-9a2f-20deb44571f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch modules\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e02db3-65bb-431b-91a7-78aabde35a12",
   "metadata": {},
   "source": [
    "# I - Load a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc4e36e-bae8-42a5-99ac-5efb0de10bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d6d58-664d-49b8-8802-cba1b85f618a",
   "metadata": {},
   "source": [
    "## FashionMNIST Dataset\n",
    "\n",
    "The **FashionMNIST** dataset is a modern replacement for the classic MNIST digits dataset. \n",
    "but instead of handwritten numbers, it contains **fashion article images** from *Zalando* (clothing items, shoes, bags, etc.).\n",
    "\n",
    "📚 More info: [PyTorch docs](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html)  \n",
    "👕 Official GitHub: [zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)\n",
    "\n",
    "The dataset contains:\n",
    "- **60,000 training images**\n",
    "- **10,000 test images**\n",
    "\n",
    "Each image is a **grayscale 28×28 pixel** image belonging to one of **10 classes** (T-shirt, Trouser, Pullover, etc.).\n",
    "\n",
    "We can notice that the `train` parameter in `torchvision.datasets.FashionMNIST` simply tells PyTorch which split to load:\n",
    "- `train=True` → training set  \n",
    "- `train=False` → test set  \n",
    "\n",
    "This means that **the dataset already comes pre-split**: we just choose which part to use.\n",
    "\n",
    "Common arguments:\n",
    "- `download=True` → downloads the dataset locally if it’s not already present.  \n",
    "- `transform=ToTensor()` → converts the images (PIL format) into PyTorch tensors so they can be processed by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26fcf870-ecae-4fd6-bfc2-22383b4c1468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540823e1-1d60-45af-93da-0ce666ea302c",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "The **DataLoader** class is used to load and iterate through a PyTorch dataset efficiently.\n",
    "\n",
    "It mainly takes two parameters:\n",
    "- a **dataset** (e.g. `train_dataset`)  \n",
    "- a **batch size** = number of samples to feed into the model per forward/backward pass.\n",
    "\n",
    "A larger batch size uses **more memory (RAM/VRAM)** but can improve training stability.\n",
    "\n",
    "The data is usually represented in the **NCHW format** (the shape of an image tensor):\n",
    "\n",
    "- **N** → number of samples in the batch  \n",
    "- **C** → number of channels (for an RGB image, C = 3; for grayscale, C = 1)  \n",
    "- **H** → image height  \n",
    "- **W** → image width  \n",
    "\n",
    "Those four axes define the dimensions of the image tensors handled by PyTorch.\n",
    "\n",
    "📖 More info on the [DataLoader documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2bd44c-0b42-459e-b44e-e3a5d0cd77b3",
   "metadata": {},
   "source": [
    "# II - Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42bf4475-6796-43b6-b802-3c7c1a2dfd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfd35e8-65a2-4dcf-a1e4-e6aba9bb9d98",
   "metadata": {},
   "source": [
    "To create a neural network in PyTorch, we **have to** create a class that inherits from **`nn.Module`**.\n",
    "[Here](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html) is the official documentation for `nn.Module`.\n",
    "\n",
    "We have the choice of using an accelerator such as **CUDA**, or sticking with the **CPU**.\n",
    "\n",
    "---\n",
    "\n",
    "### Defining the Neural Network\n",
    "\n",
    "In our class constructor, we first define `nn.Flatten()`.\n",
    "\n",
    "This layer converts each 2D 28×28 image into a contiguous array of 784 pixel values.\n",
    "A **contiguous array** is an array stored in an **unbroken block of memory**. [Here](https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays) is an illustrated explanation.\n",
    "\n",
    "Next, we define a **`nn.Sequential`** attribute called `self.linear_relu_stack`.\n",
    "\n",
    "A **Sequential** layer is a container that allows data to pass sequentially through multiple layers, in our case a mix of linear and ReLU layers.\n",
    "[Here](https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) is more information about the Sequential module.\n",
    "\n",
    "---\n",
    "\n",
    "### Layers Inside `nn.Sequential`\n",
    "\n",
    "* **`nn.Linear`**: A linear layer applies a linear transformation (y = Ax + b) on the input using its stored weights and biases.\n",
    "* **`nn.ReLU`**: A non-linear activation layer that applies (y = 0) if (x \\le 0), else (y = x). [More details](https://docs.pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "\n",
    "---\n",
    "\n",
    "### Constructor and Forward Method\n",
    "\n",
    "The constructor `__init__` is called **once** when the neural network is created.\n",
    "At this point, all layers (flatten and sequential) are just defined, they are not yet applied to any data.\n",
    "\n",
    "The **`forward(self, x)`** method is called **every time** data is passed through the network.\n",
    "\n",
    "* The input `x` is first **flattened**.\n",
    "* Then it passes through `self.linear_relu_stack` (the linear and ReLU layers).\n",
    "\n",
    "The output of this method is called **logits**, which are the **unnormalized outputs** of the model.\n",
    "We often normalize them using a **softmax** function.\n",
    "\n",
    "Logits are the prediction of our model, it need to be compared to the target value to improve our model.\n",
    "\n",
    "With logits, we can define a **loss function** and perform **backpropagation** to train the network's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c734154-757d-4fb8-8ccf-c8dccdefd733",
   "metadata": {},
   "source": [
    "# III - Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d94ee2c6-928d-467e-a3e1-90781e35e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc5579-00f5-48e8-88a1-bdb662fedaa8",
   "metadata": {},
   "source": [
    "To train model parameters, we need to define both a **loss function** and an **optimizer**.\n",
    "\n",
    "In our case, we’ll use the **Cross Entropy** loss, one of the most common choices among the many different [loss functions available in PyTorch](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "---\n",
    "\n",
    "## A quick explanation about the most popular loss functions\n",
    "\n",
    "For all the following formulas:\n",
    "- $y_i$ = target value  \n",
    "- $\\hat{y}_i$ = predicted value  \n",
    "- $N$ = number of samples\n",
    "\n",
    "---\n",
    "\n",
    "### 1 - Mean Squared Error (MSELoss)\n",
    "\n",
    "Used for **regression tasks**, when predicting continuous values (e.g. temperature, house prices, etc.).\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\bigl(y_i - \\hat{y}_i\\bigr)^2\n",
    "$$\n",
    "\n",
    "MSE penalizes large errors more strongly because we take into consideration the **square value** of the difference between prediction and target.\n",
    "\n",
    "![MSE](https://miro.medium.com/v2/resize:fit:640/format:webp/1*WfVDoLsarrM5HpO9sh_ZQQ.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 2 - Mean Absolute Error (L1Loss)\n",
    "\n",
    "Used for **regression** when you want **robustness to outliers** (*valeurs aberrantes*).\n",
    "\n",
    "$$\n",
    "\\mathrm{MAE}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\bigl|y_i - \\hat{y}_i\\bigr|\n",
    "$$\n",
    "\n",
    "Unlike MSE, it applies a **linear penalty** instead of a squared one.  \n",
    "That makes it less sensitive to outliers, but the optimization landscape is less smooth which can lead to sparse gradients, making it harder to optimize.\n",
    "\n",
    "![L1_loss](https://miro.medium.com/v2/resize:fit:640/format:webp/1*0hbNOtpfr6aoR_Bmty-JkA.jpeg)\n",
    "\n",
    "---\n",
    "\n",
    "### 3 - Cross Entropy Loss\n",
    "\n",
    "The **Cross Entropy Loss** is used for **multi-class classification** problems.\n",
    "\n",
    "For each input, the model outputs a set of **scores (logits)**, one per class.  \n",
    "We then convert those scores into probabilities that sum to 1 using the **Softmax** function (e.g. 0.3 → 30% dog, 0.5 → 50% cat, etc.).\n",
    "\n",
    "The loss compares the predicted probabilities to the true class label, which is represented as a **“perfect” distribution** (1 for the correct class, 0 for the others).\n",
    "\n",
    "$$\n",
    "\\mathrm{CrossEntropy}(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log\\left( \\frac{e^{\\hat{y}_{i, y_i}}}{\\sum_{j} e^{\\hat{y}_{i,j}}} \\right)\n",
    "$$\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "1. Apply **Softmax** to convert logits into probabilities:\n",
    "\n",
    "$$\n",
    "p_{i,j} = \\frac{e^{\\hat{y}_{i,j}}}{\\sum_{k} e^{\\hat{y}_{i,k}}}\n",
    "$$\n",
    "\n",
    "2. Then compute the average **negative log-likelihood** of the correct class:\n",
    "\n",
    "$$\n",
    "\\mathrm{CrossEntropy}(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log\\left(p_{i, y_i}\\right)\n",
    "$$\n",
    "\n",
    "This encourages the model to assign higher probabilities to the correct class.\n",
    "\n",
    "![cross_entropy](https://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 4 - Binary Cross-Entropy (BCE)\n",
    "\n",
    "Used for **binary classification** (e.g. spam vs. not spam).\n",
    "\n",
    "$$\n",
    "\\mathrm{BCE}(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- The target $y_i$ ∈ {0, 1}  \n",
    "- The prediction $\\hat{y}_i \\in [0, 1]$ represents the **probability** of belonging to class 1 (e.g. class spam = 1, not spam = 0).\n",
    "\n",
    "To get $\\hat{y}_i$, we apply a **sigmoid** to the raw model output (logit):\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "If you want to work directly with logits (without applying sigmoid yourself), use `nn.BCEWithLogitsLoss`, which combines both operations safely.\n",
    "\n",
    "![binary_Xentropy](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F54c97fda8af4dccc23d58bd14cd95802df6f1e49-393x272.png&w=640&q=75)\n",
    "\n",
    "---\n",
    "\n",
    "### 5 - Negative Log Likelihood Loss (NLLLoss)\n",
    "\n",
    "Used for **multi-class classification** when your model already outputs **log-probabilities** instead of raw logits.\n",
    "\n",
    "$$\n",
    "\\mathrm{NLL}(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p_{i, y_i}\n",
    "$$\n",
    "\n",
    "This is mathematically equivalent to the **Cross Entropy Loss**, but expects the input to already be log-softmaxed.\n",
    "\n",
    "In PyTorch, that’s done using `nn.LogSoftmax`:\n",
    "\n",
    "$$\n",
    "\\log p_{i,j} = \\hat{y}_{i,j} - \\log \\left( \\sum_{k=1}^{C} e^{\\hat{y}_{i,k}} \\right)\n",
    "$$\n",
    "\n",
    "Substituting it gives:\n",
    "\n",
    "$$\n",
    "\\mathrm{NLL}(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( \\hat{y}_{i, y_i} - \\log \\sum_{k=1}^{C} e^{\\hat{y}_{i,k}} \\right)\n",
    "$$\n",
    "\n",
    "In practice, we rarely use NLLLoss directly because **`CrossEntropyLoss` already combines `LogSoftmax` and `NLLLoss`** for better numerical stability.\n",
    "\n",
    "---\n",
    "\n",
    "### 6 - Huber Loss (`nn.SmoothL1Loss`)\n",
    "\n",
    "Used for **regression** tasks with both small and large errors.  \n",
    "It’s a compromise between **MSE** (sensitive to outliers) and **MAE** (robust but non-smooth).\n",
    "\n",
    "$$\n",
    "L_{\\delta}(y, \\hat{y}) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{2} (y - \\hat{y})^2, & \\text{if } |y - \\hat{y}| \\le \\delta, \\\\\n",
    "\\delta \\cdot \\bigl(|y - \\hat{y}| - \\tfrac{1}{2}\\delta \\bigr), & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The parameter $\\delta$ defines the transition point between quadratic and linear loss.  \n",
    "You can tune it experimentally to get the best fit for your dataset.\n",
    "\n",
    "**Huber Loss** is a good general-purpose choice when you want stability and robustness in regression.\n",
    "\n",
    "---\n",
    "\n",
    "### 7 - KL Divergence Loss (`nn.KLDivLoss`)\n",
    "\n",
    "Used for comparing **probability distributions** — for example, in **Variational Autoencoders** or **Knowledge Distillation**.\n",
    "\n",
    "It measures how one probability distribution $P$ diverges from another $Q$:\n",
    "\n",
    "$$\n",
    "D_{KL}(P || Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n",
    "In simple terms, it tells us how much information is lost when we use $Q$ to approximate $P$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f2b408-963a-4e50-9437-df5d002c191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04738886-324d-4d8a-b7af-7ecc706882f4",
   "metadata": {},
   "source": [
    "## A quick explanation about the optimizer\n",
    "\n",
    "An **optimizer** is the algorithm that updates my model's parameters (weights and biases) based on the computed gradients during training.\n",
    "\n",
    "During one training pass we have:\n",
    "- the **forward** method that generates the logits (the model’s predictions),\n",
    "- the **loss** function that computes the error,\n",
    "- a **backward pass** (`loss.backward()`) that calculates the gradients,\n",
    "- and finally the **optimizer step** (`optimizer.step()`) that updates the weights.\n",
    "\n",
    "The loss will be designated by the variable $L$. Computing the partial derivative of our loss function with respect of the parameter $\\theta_i$ is written $\\dfrac{\\partial L}{\\partial \\theta_i}$ and tell how much and in which direction will the loss change when changing the parameter $\\theta_i$. This mathematical notion is crucial for what is following.\n",
    "\n",
    "---\n",
    "\n",
    "### 1 - Stochastic Gradient Descent (SGD)\n",
    "\n",
    "This is the **simplest** and most classic optimizer.\n",
    "\n",
    "**Stochastic** means it uses a random subset of data (a *batch*) instead of the full dataset for each update.  \n",
    "If we use the entire dataset at once, that’s **Batch Gradient Descent**.  \n",
    "If we use only one example at a time, that’s **Pure SGD**.  \n",
    "In practice, we use small batches (like 64 samples), so this is called **Mini-batch SGD**.\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\, \\nabla_\\theta L(\\theta_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta_t$ are the model parameters (weights) at step *t*,\n",
    "- $\\eta$ is the **learning rate**,  \n",
    "- and $\\nabla_\\theta L(\\theta_t)$ is the gradient of the loss with respect to the parameters.\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta L(\\theta_t) = \n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial L}{\\partial \\theta_1} \\\\\n",
    "\\dfrac{\\partial L}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{\\partial L}{\\partial \\theta_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In PyTorch, using `torch.optim.SGD()` requires:\n",
    "- the model parameters (`model.parameters()`)\n",
    "- and the learning rate `lr`.\n",
    "\n",
    "The learning rate controls how big each update step is.  \n",
    "We usually find the best `lr` experimentally:\n",
    "- `lr = 1e-3` → small, gentle updates  \n",
    "- `lr = 1e-1` → large, aggressive updates\n",
    "\n",
    "**Limitations:**\n",
    "- Same learning rate $\\eta$ for all parameters  \n",
    "- Sensitive to gradient scale  \n",
    "- Can oscillate and converge slowly, especially in deep networks  \n",
    "\n",
    "Because of those limitations, plain SGD is mostly used for **small or simple models**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2 - SGD with Momentum\n",
    "\n",
    "We can improve SGD by adding **momentum** to make training faster and smoother.\n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) \\, \\nabla_\\theta L(\\theta_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta v_t\n",
    "$$\n",
    "\n",
    "Here, $v_t$ is a **velocity** term that accumulates previous gradients.  \n",
    "It helps reduce oscillations and accelerates learning in consistent directions.\n",
    "\n",
    "We usually set the momentum coefficient $\\beta = 0.9$.\n",
    "\n",
    "**Limitations:**\n",
    "- Still uses a global learning rate for all parameters (no per-parameter adaptation)  \n",
    "- Can be sensitive to the choice of learning rate and momentum  \n",
    "\n",
    "**When to use it:**\n",
    "- Common for **CNNs**  \n",
    "  - Stable and smooth updates on dense loss surfaces  \n",
    "  - Often generalizes better than adaptive methods  \n",
    "  - Memory efficient (stores only one extra value per parameter)  \n",
    "  - Works well with learning-rate schedules (step decay, cosine annealing)\n",
    "\n",
    "---\n",
    "\n",
    "### 3 - Adagrad\n",
    "\n",
    "$$\n",
    "G_t = G_{t-1} + (\\nabla_\\theta L(\\theta_t))^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\varepsilon}} \\, \\nabla_\\theta L(\\theta_t)\n",
    "$$\n",
    "\n",
    "Here, $G_t$ is a **running sum of squared gradients**, and $\\varepsilon$ is a small constant to prevent division by zero.\n",
    "\n",
    "Each parameter keeps its own $G_t$, which means **each parameter has its own learning rate**.  \n",
    "As $G_t$ accumulates over time, the denominator grows and the effective learning rate decreases.\n",
    "\n",
    "This makes Adagrad great when gradients are **sparse**, such as in **NLP embeddings** or **recommendation systems**.\n",
    "\n",
    "**Limitation:**  \n",
    "Because $G_t$ keeps increasing, the learning rate can shrink too much, stopping training prematurely.\n",
    "\n",
    "---\n",
    "\n",
    "### 4 - RMSProp\n",
    "\n",
    "RMSProp fixes Adagrad’s limitation by keeping an **exponentially decaying average** of past squared gradients instead of a cumulative sum.\n",
    "\n",
    "$$\n",
    "v_t = \\alpha v_{t-1} + (1 - \\alpha) \\, (\\nabla_\\theta L(\\theta_t))^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{v_t + \\varepsilon}} \\, \\nabla_\\theta L(\\theta_t)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $v_t$ is the exponentially weighted moving average of squared gradients,  \n",
    "- $\\alpha$ (typically 0.9) controls the decay rate.\n",
    "\n",
    "We keep about 90% of the old value and add 10% of the new squared gradient.  \n",
    "This keeps $v_t$ stable and prevents learning rates from vanishing, unlike Adagrad.\n",
    "\n",
    "RMSProp is often used in **RNNs**, where gradient magnitudes vary a lot.\n",
    "\n",
    "---\n",
    "\n",
    "### 5 - Adaptive Moment Estimation (Adam)\n",
    "\n",
    "Adam combines the ideas of **Momentum** and **RMSProp**:\n",
    "- it keeps a running average of past gradients $m_t$ (the **first moment**),\n",
    "- and a running average of squared gradients $v_t$ (the **second moment**).\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\, \\nabla_\\theta L(\\theta_t)\n",
    "$$\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\, (\\nabla_\\theta L(\\theta_t))^2\n",
    "$$\n",
    "\n",
    "Both $m_t$ and $v_t$ start at zero, so early in training they are biased toward 0.  \n",
    "We correct this with **bias-corrected estimates**:\n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \n",
    "\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "Then the parameters are updated as:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- The numerator $\\hat{m}_t$ gives the update **direction** (the mean of past gradients),\n",
    "- The denominator $\\sqrt{\\hat{v}_t}$ scales the step size according to gradient **variance**,\n",
    "- $\\varepsilon$ (usually $10^{-8}$) prevents division by zero.\n",
    "\n",
    "Because $\\beta_1$ and $\\beta_2$ are close to 1 (usually 0.9 and 0.999),  \n",
    "the bias correction is strong at the beginning and fades over time ($\\beta^{t+1} < \\beta^t$), letting $m_t$ and $v_t$ settle to their true averages.\n",
    "\n",
    "Adam adapts the learning rate per parameter and smooths updates, making it robust to noisy gradients and suitable for most architectures.\n",
    "\n",
    "This optimizer is the **default choice** for most deep learning models (CNNs, RNNs, Transformers) because it converges fast and needs little tuning.\n",
    "\n",
    "---\n",
    "\n",
    "### 6 - AdamW (Adam with Decoupled Weight Decay)\n",
    "\n",
    "In the original Adam, **weight decay** was coupled with the gradient update, which caused issues for large models.  \n",
    "**AdamW** decouples weight decay from the adaptive update:\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\, \\nabla_\\theta L(\\theta_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\, (\\nabla_\\theta L(\\theta_t))^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad\n",
    "\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\left( \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon} + \\lambda \\theta_t \\right)\n",
    "$$\n",
    "\n",
    "Only the final update changes — the term $\\eta \\lambda \\theta_t$ directly penalizes large weights.  \n",
    "The weight-decay coefficient $\\lambda$ is typically set around $10^{-2}$.\n",
    "\n",
    "AdamW leads to **better generalization** for large models such as **Transformers**, **LLMs**, and **large-scale vision models**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ae57de-b33d-48e2-9457-fa5e31ec3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d341288-e8b5-47ed-9dfd-94389de3a69e",
   "metadata": {},
   "source": [
    "In the dataloader, we defined the batch size to 64 — so we’re using **Mini-batch SGD**, as seen previously.\n",
    "\n",
    "In the `train(dataloader, model, loss_fn, optimizer)` function, here’s what happens step by step:\n",
    "\n",
    "* **`size = len(dataloader.dataset)`**\n",
    "  Retrieves the total number of samples in the dataset.\n",
    "\n",
    "* **`model.train()`**\n",
    "  Sets the model in **training mode**.\n",
    "  Some layers like `Dropout` or `BatchNorm` behave differently during training and evaluation, so this line is essential before starting a training pass.\n",
    "\n",
    "* **`for batch, (X, y) in enumerate(dataloader):`**\n",
    "  Iterates over each batch of data.\n",
    "  Since we’re using `enumerate`, `batch` is just the batch index.\n",
    "  `X` contains the input tensors (shape $[N, C, H, W]$ for images), and `y` contains the corresponding labels (shape $[N]$ for classification tasks).\n",
    "\n",
    "* **`X, y = X.to(device), y.to(device)`**\n",
    "  Moves the inputs and labels to the same device as the model (CPU or GPU).\n",
    "  This step is mandatory if the model is on GPU, otherwise you’ll get a device mismatch error.\n",
    "\n",
    "* **`pred = model(X)`**\n",
    "  Performs a **forward pass** through the model.\n",
    "  This line implicitly calls the model’s `forward(self, X)` method and returns the raw outputs (called **logits** in classification tasks).\n",
    "\n",
    "* **`loss = loss_fn(pred, y)`**\n",
    "  Computes the **loss value**, i.e. how far the predictions are from the true labels.\n",
    "  For example, if we use cross-entropy, this compares the predicted probabilities to the true class labels.\n",
    "  The result is a single scalar value.\n",
    "\n",
    "* **`loss.backward()`**\n",
    "  This triggers **automatic differentiation** using backpropagation.\n",
    "  PyTorch computes the gradient of the loss with respect to each parameter in the model:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial \\theta_i}\n",
    "  $$\n",
    "\n",
    "  These gradients are then stored in the `.grad` attribute of each parameter that has `requires_grad=True`.\n",
    "\n",
    "* **`optimizer.step()`**\n",
    "  The optimizer uses those gradients to **update the model’s parameters** according to the optimization rule (for example, SGD or Adam).\n",
    "  This is where learning actually happens.\n",
    "\n",
    "* **`optimizer.zero_grad()`**\n",
    "  Clears the previously stored gradients in `.grad`.\n",
    "  This avoids **gradient accumulation** from multiple backward passes, which would otherwise cause incorrect updates.\n",
    "\n",
    "Finally, the rest of the code simply **prints the loss every 100 batches** to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d85d457-ea87-4c42-b5b0-6d52f866ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12eb32-2037-428c-a0b2-2a7f416bdd66",
   "metadata": {},
   "source": [
    "The `test(dataloader, model, loss_fn)` function is an evaluation loop. It check how well the model performs on unseen data i.e. the test dataset.\n",
    "\n",
    "There is some parts of this function that need explaination:\n",
    "\n",
    "- **`model.eval()`**: It switch the model to evaluation mode, same as `model.train()`does it for the training mode. It is crucial for some layers like `Dropout` or `BatchNorm `\n",
    "\n",
    "- **`with torch.no_grad()`**: it tells Pytorch **not to track gradients** or store intermediate values for backpropagation. Since we are only testing and not training, we do not need gradient. Doing so reduce memory usage and speeds up computation.\n",
    "\n",
    "- **`test_loss += loss_fn(pred, y).item()`**: it computes the loss function like in the train function. `item()`extract the scalar value from the tensor.\n",
    "\n",
    "- **`correct += (pred.argmax(1) == y).type(torch.float).sum().item()`**:\n",
    "\n",
    "    -  `pred.argmax(1)` takes the index of the highest logit along dimension 1 i.e. the predicted class, the one with the best score.\n",
    "    -  `== y` compare the prediction with the target value, return a tensor of booleans.\n",
    "    -  `.type(torch.float)` converts booleans to floats\n",
    "    -  `.sum().item()` sums them to get the number of correct preditions in this batch\n",
    "    -  `correct +=` accumulate the number of correct predictions over alll batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5d853f7-0195-4fab-bad0-fe61bbcacb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.296837  [   64/60000]\n",
      "loss: 2.292622  [ 6464/60000]\n",
      "loss: 2.270679  [12864/60000]\n",
      "loss: 2.273979  [19264/60000]\n",
      "loss: 2.249237  [25664/60000]\n",
      "loss: 2.230193  [32064/60000]\n",
      "loss: 2.225190  [38464/60000]\n",
      "loss: 2.196285  [44864/60000]\n",
      "loss: 2.198832  [51264/60000]\n",
      "loss: 2.181857  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.4%, Avg loss: 2.165192 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.169605  [   64/60000]\n",
      "loss: 2.165305  [ 6464/60000]\n",
      "loss: 2.109302  [12864/60000]\n",
      "loss: 2.130728  [19264/60000]\n",
      "loss: 2.080288  [25664/60000]\n",
      "loss: 2.032074  [32064/60000]\n",
      "loss: 2.043264  [38464/60000]\n",
      "loss: 1.978059  [44864/60000]\n",
      "loss: 1.987979  [51264/60000]\n",
      "loss: 1.921017  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.912943 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.941181  [   64/60000]\n",
      "loss: 1.917256  [ 6464/60000]\n",
      "loss: 1.802822  [12864/60000]\n",
      "loss: 1.838444  [19264/60000]\n",
      "loss: 1.735825  [25664/60000]\n",
      "loss: 1.692222  [32064/60000]\n",
      "loss: 1.691929  [38464/60000]\n",
      "loss: 1.607263  [44864/60000]\n",
      "loss: 1.630352  [51264/60000]\n",
      "loss: 1.518563  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.536036 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.600397  [   64/60000]\n",
      "loss: 1.567984  [ 6464/60000]\n",
      "loss: 1.413851  [12864/60000]\n",
      "loss: 1.478250  [19264/60000]\n",
      "loss: 1.365276  [25664/60000]\n",
      "loss: 1.363341  [32064/60000]\n",
      "loss: 1.361173  [38464/60000]\n",
      "loss: 1.297258  [44864/60000]\n",
      "loss: 1.333888  [51264/60000]\n",
      "loss: 1.230806  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.256152 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.330422  [   64/60000]\n",
      "loss: 1.319630  [ 6464/60000]\n",
      "loss: 1.146374  [12864/60000]\n",
      "loss: 1.246456  [19264/60000]\n",
      "loss: 1.128349  [25664/60000]\n",
      "loss: 1.155275  [32064/60000]\n",
      "loss: 1.166854  [38464/60000]\n",
      "loss: 1.112075  [44864/60000]\n",
      "loss: 1.152664  [51264/60000]\n",
      "loss: 1.073739  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.089812 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.155756  [   64/60000]\n",
      "loss: 1.170040  [ 6464/60000]\n",
      "loss: 0.978026  [12864/60000]\n",
      "loss: 1.108670  [19264/60000]\n",
      "loss: 0.987657  [25664/60000]\n",
      "loss: 1.021070  [32064/60000]\n",
      "loss: 1.051176  [38464/60000]\n",
      "loss: 0.997281  [44864/60000]\n",
      "loss: 1.037120  [51264/60000]\n",
      "loss: 0.979156  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.985360 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.037397  [   64/60000]\n",
      "loss: 1.075543  [ 6464/60000]\n",
      "loss: 0.865032  [12864/60000]\n",
      "loss: 1.018702  [19264/60000]\n",
      "loss: 0.900656  [25664/60000]\n",
      "loss: 0.927527  [32064/60000]\n",
      "loss: 0.976565  [38464/60000]\n",
      "loss: 0.922290  [44864/60000]\n",
      "loss: 0.957551  [51264/60000]\n",
      "loss: 0.916115  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.914298 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.950830  [   64/60000]\n",
      "loss: 1.009972  [ 6464/60000]\n",
      "loss: 0.783619  [12864/60000]\n",
      "loss: 0.954511  [19264/60000]\n",
      "loss: 0.842499  [25664/60000]\n",
      "loss: 0.858783  [32064/60000]\n",
      "loss: 0.923841  [38464/60000]\n",
      "loss: 0.871013  [44864/60000]\n",
      "loss: 0.899805  [51264/60000]\n",
      "loss: 0.870362  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.862601 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.884136  [   64/60000]\n",
      "loss: 0.960058  [ 6464/60000]\n",
      "loss: 0.722173  [12864/60000]\n",
      "loss: 0.905866  [19264/60000]\n",
      "loss: 0.800586  [25664/60000]\n",
      "loss: 0.806604  [32064/60000]\n",
      "loss: 0.883415  [38464/60000]\n",
      "loss: 0.834566  [44864/60000]\n",
      "loss: 0.856407  [51264/60000]\n",
      "loss: 0.834898  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.822993 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.830778  [   64/60000]\n",
      "loss: 0.919439  [ 6464/60000]\n",
      "loss: 0.673989  [12864/60000]\n",
      "loss: 0.867714  [19264/60000]\n",
      "loss: 0.768394  [25664/60000]\n",
      "loss: 0.766142  [32064/60000]\n",
      "loss: 0.850401  [38464/60000]\n",
      "loss: 0.806900  [44864/60000]\n",
      "loss: 0.822534  [51264/60000]\n",
      "loss: 0.806040  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.791270 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.786609  [   64/60000]\n",
      "loss: 0.884553  [ 6464/60000]\n",
      "loss: 0.635095  [12864/60000]\n",
      "loss: 0.837082  [19264/60000]\n",
      "loss: 0.742325  [25664/60000]\n",
      "loss: 0.734057  [32064/60000]\n",
      "loss: 0.822071  [38464/60000]\n",
      "loss: 0.785096  [44864/60000]\n",
      "loss: 0.795356  [51264/60000]\n",
      "loss: 0.781741  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.764942 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.749184  [   64/60000]\n",
      "loss: 0.853781  [ 6464/60000]\n",
      "loss: 0.602932  [12864/60000]\n",
      "loss: 0.811932  [19264/60000]\n",
      "loss: 0.720540  [25664/60000]\n",
      "loss: 0.708334  [32064/60000]\n",
      "loss: 0.796935  [38464/60000]\n",
      "loss: 0.767062  [44864/60000]\n",
      "loss: 0.772869  [51264/60000]\n",
      "loss: 0.760569  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.742369 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.716810  [   64/60000]\n",
      "loss: 0.825987  [ 6464/60000]\n",
      "loss: 0.575816  [12864/60000]\n",
      "loss: 0.790785  [19264/60000]\n",
      "loss: 0.701990  [25664/60000]\n",
      "loss: 0.687298  [32064/60000]\n",
      "loss: 0.773944  [38464/60000]\n",
      "loss: 0.751303  [44864/60000]\n",
      "loss: 0.753761  [51264/60000]\n",
      "loss: 0.741655  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.722500 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.688382  [   64/60000]\n",
      "loss: 0.800624  [ 6464/60000]\n",
      "loss: 0.552529  [12864/60000]\n",
      "loss: 0.772527  [19264/60000]\n",
      "loss: 0.685854  [25664/60000]\n",
      "loss: 0.669678  [32064/60000]\n",
      "loss: 0.752500  [38464/60000]\n",
      "loss: 0.737173  [44864/60000]\n",
      "loss: 0.737198  [51264/60000]\n",
      "loss: 0.724344  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.704678 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.663091  [   64/60000]\n",
      "loss: 0.777333  [ 6464/60000]\n",
      "loss: 0.532170  [12864/60000]\n",
      "loss: 0.756506  [19264/60000]\n",
      "loss: 0.671609  [25664/60000]\n",
      "loss: 0.654761  [32064/60000]\n",
      "loss: 0.732480  [38464/60000]\n",
      "loss: 0.724373  [44864/60000]\n",
      "loss: 0.722582  [51264/60000]\n",
      "loss: 0.708280  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.688510 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.640488  [   64/60000]\n",
      "loss: 0.755891  [ 6464/60000]\n",
      "loss: 0.514162  [12864/60000]\n",
      "loss: 0.742308  [19264/60000]\n",
      "loss: 0.658994  [25664/60000]\n",
      "loss: 0.641906  [32064/60000]\n",
      "loss: 0.713666  [38464/60000]\n",
      "loss: 0.712761  [44864/60000]\n",
      "loss: 0.709617  [51264/60000]\n",
      "loss: 0.693184  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.673739 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.620180  [   64/60000]\n",
      "loss: 0.736203  [ 6464/60000]\n",
      "loss: 0.498190  [12864/60000]\n",
      "loss: 0.729559  [19264/60000]\n",
      "loss: 0.647964  [25664/60000]\n",
      "loss: 0.630721  [32064/60000]\n",
      "loss: 0.696110  [38464/60000]\n",
      "loss: 0.702436  [44864/60000]\n",
      "loss: 0.698058  [51264/60000]\n",
      "loss: 0.678954  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.660189 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.601850  [   64/60000]\n",
      "loss: 0.717998  [ 6464/60000]\n",
      "loss: 0.483873  [12864/60000]\n",
      "loss: 0.717884  [19264/60000]\n",
      "loss: 0.638041  [25664/60000]\n",
      "loss: 0.621104  [32064/60000]\n",
      "loss: 0.679618  [38464/60000]\n",
      "loss: 0.693177  [44864/60000]\n",
      "loss: 0.687885  [51264/60000]\n",
      "loss: 0.665543  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.647715 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.585083  [   64/60000]\n",
      "loss: 0.701196  [ 6464/60000]\n",
      "loss: 0.471045  [12864/60000]\n",
      "loss: 0.707054  [19264/60000]\n",
      "loss: 0.629073  [25664/60000]\n",
      "loss: 0.612587  [32064/60000]\n",
      "loss: 0.664228  [38464/60000]\n",
      "loss: 0.685085  [44864/60000]\n",
      "loss: 0.678959  [51264/60000]\n",
      "loss: 0.652947  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.636211 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.569708  [   64/60000]\n",
      "loss: 0.685803  [ 6464/60000]\n",
      "loss: 0.459453  [12864/60000]\n",
      "loss: 0.696964  [19264/60000]\n",
      "loss: 0.621016  [25664/60000]\n",
      "loss: 0.605090  [32064/60000]\n",
      "loss: 0.649882  [38464/60000]\n",
      "loss: 0.678034  [44864/60000]\n",
      "loss: 0.671155  [51264/60000]\n",
      "loss: 0.641024  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.625590 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7659547e-cf6b-409e-a607-766c969a7a89",
   "metadata": {},
   "source": [
    "If needed, we can save the model with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "912893c3-6ce6-4788-980c-818eeacde3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
