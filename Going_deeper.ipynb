{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c553872-9488-457b-9a16-2560cea849bc",
   "metadata": {},
   "source": [
    "# Going deeper with our models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b91733-b6e8-4b6a-a6d3-b72295dafd96",
   "metadata": {},
   "source": [
    "Now that we had a good overview on the different steps on how to build our model, we will see some **tips** and **tools** when training our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6eb73-6925-41ab-96c9-d150d279183c",
   "metadata": {},
   "source": [
    "## Epochs and loss\n",
    "\n",
    "We have cover the differents lost functions and talk about the general idea: we need to minimize the loss to have better results.\n",
    "At the end of the `Quickstart.ipynb` we introduced the concept of Epochs. We defined it as the number of pass we will have throught the entire training dataset.\n",
    "Howerver, we just gave a fixed number of epochs without specifying why we choose this one and if it is really the best one.\n",
    "\n",
    "Finding the number of epochs is experimental, as long as the validation loss improve, we can go through new epochs.\n",
    "If the validation loss starts increasing while the training loss keeps decreasing, we have to stop training our model, it is **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39198a9c-cb51-45b6-b208-63473e9cb189",
   "metadata": {},
   "source": [
    "## Tools for visualization\n",
    "\n",
    "To see in real time the validation and training loss, the **TensorBoard** tool is often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d33a13-63fc-45de-9147-968d1ecf8c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b4d4ee3-39e2-4dd2-ac26-6eb10c63752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "torch.set_num_threads(16)\n",
    "\n",
    "# -----------------------------\n",
    "# Define model\n",
    "# -----------------------------\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Define pipeline\n",
    "# -----------------------------\n",
    "class ModelPipeline:\n",
    "    def __init__(self):\n",
    "        # Data\n",
    "        self.training_data = datasets.FashionMNIST(\n",
    "            root=\"data\", train=True, download=True, transform=ToTensor()\n",
    "        )\n",
    "        self.test_data = datasets.FashionMNIST(\n",
    "            root=\"data\", train=False, download=True, transform=ToTensor()\n",
    "        )\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.train_dataloader = DataLoader(self.training_data, batch_size=self.batch_size, shuffle=True)\n",
    "        self.test_dataloader = DataLoader(self.test_data, batch_size=self.batch_size)\n",
    "\n",
    "        # Device\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using {self.device} device\")\n",
    "\n",
    "        # Model, loss, optimizer\n",
    "        self.model = NeuralNetwork().to(self.device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        total_loss, correct = 0, 0\n",
    "\n",
    "        for X, y in self.train_dataloader:\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.train_dataloader)\n",
    "        accuracy = correct / len(self.training_data)\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        total_loss, correct = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.test_dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self.model(X)\n",
    "                total_loss += self.loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.test_dataloader)\n",
    "        accuracy = correct / len(self.test_data)\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def run(self, epochs=20):\n",
    "        run_name = f\"fashionMNIST_SGD_lr1e-3_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "        writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            train_loss, train_acc = self.train()\n",
    "            val_loss, val_acc = self.test()\n",
    "\n",
    "            writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "            writer.add_scalar(\"Loss/validation\", val_loss, epoch)\n",
    "            writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "            writer.add_scalar(\"Accuracy/validation\", val_acc, epoch)\n",
    "\n",
    "            print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f}\")\n",
    "            print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.3f}\\n\")\n",
    "\n",
    "        writer.close()\n",
    "        print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf2ea4c6-a794-4d41-a3d0-04387813c3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 243876), started 0:07:27 ago. (Use '!kill 243876' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f537679ef4b9403b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f537679ef4b9403b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae17af65-a0fc-4ac5-9fb3-93af55531c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Epoch 1/80\n",
      "  Train Loss: 2.2365 | Train Acc: 0.372\n",
      "  Val   Loss: 2.1636 | Val   Acc: 0.519\n",
      "\n",
      "Epoch 2/80\n",
      "  Train Loss: 2.0485 | Train Acc: 0.590\n",
      "  Val   Loss: 1.9094 | Val   Acc: 0.618\n",
      "\n",
      "Epoch 3/80\n",
      "  Train Loss: 1.7246 | Train Acc: 0.633\n",
      "  Val   Loss: 1.5477 | Val   Acc: 0.616\n",
      "\n",
      "Epoch 4/80\n",
      "  Train Loss: 1.3941 | Train Acc: 0.634\n",
      "  Val   Loss: 1.2722 | Val   Acc: 0.638\n",
      "\n",
      "Epoch 5/80\n",
      "  Train Loss: 1.1715 | Train Acc: 0.650\n",
      "  Val   Loss: 1.0991 | Val   Acc: 0.647\n",
      "\n",
      "Epoch 6/80\n",
      "  Train Loss: 1.0307 | Train Acc: 0.664\n",
      "  Val   Loss: 0.9885 | Val   Acc: 0.661\n",
      "\n",
      "Epoch 7/80\n",
      "  Train Loss: 0.9385 | Train Acc: 0.677\n",
      "  Val   Loss: 0.9137 | Val   Acc: 0.671\n",
      "\n",
      "Epoch 8/80\n",
      "  Train Loss: 0.8738 | Train Acc: 0.690\n",
      "  Val   Loss: 0.8604 | Val   Acc: 0.684\n",
      "\n",
      "Epoch 9/80\n",
      "  Train Loss: 0.8263 | Train Acc: 0.701\n",
      "  Val   Loss: 0.8203 | Val   Acc: 0.702\n",
      "\n",
      "Epoch 10/80\n",
      "  Train Loss: 0.7897 | Train Acc: 0.715\n",
      "  Val   Loss: 0.7885 | Val   Acc: 0.705\n",
      "\n",
      "Epoch 11/80\n",
      "  Train Loss: 0.7598 | Train Acc: 0.725\n",
      "  Val   Loss: 0.7617 | Val   Acc: 0.721\n",
      "\n",
      "Epoch 12/80\n",
      "  Train Loss: 0.7345 | Train Acc: 0.736\n",
      "  Val   Loss: 0.7391 | Val   Acc: 0.732\n",
      "\n",
      "Epoch 13/80\n",
      "  Train Loss: 0.7128 | Train Acc: 0.746\n",
      "  Val   Loss: 0.7190 | Val   Acc: 0.740\n",
      "\n",
      "Epoch 14/80\n",
      "  Train Loss: 0.6928 | Train Acc: 0.756\n",
      "  Val   Loss: 0.7007 | Val   Acc: 0.749\n",
      "\n",
      "Epoch 15/80\n",
      "  Train Loss: 0.6747 | Train Acc: 0.763\n",
      "  Val   Loss: 0.6840 | Val   Acc: 0.759\n",
      "\n",
      "Epoch 16/80\n",
      "  Train Loss: 0.6585 | Train Acc: 0.771\n",
      "  Val   Loss: 0.6688 | Val   Acc: 0.766\n",
      "\n",
      "Epoch 17/80\n",
      "  Train Loss: 0.6436 | Train Acc: 0.778\n",
      "  Val   Loss: 0.6547 | Val   Acc: 0.772\n",
      "\n",
      "Epoch 18/80\n",
      "  Train Loss: 0.6295 | Train Acc: 0.784\n",
      "  Val   Loss: 0.6421 | Val   Acc: 0.776\n",
      "\n",
      "Epoch 19/80\n",
      "  Train Loss: 0.6172 | Train Acc: 0.789\n",
      "  Val   Loss: 0.6310 | Val   Acc: 0.781\n",
      "\n",
      "Epoch 20/80\n",
      "  Train Loss: 0.6054 | Train Acc: 0.793\n",
      "  Val   Loss: 0.6204 | Val   Acc: 0.785\n",
      "\n",
      "Epoch 21/80\n",
      "  Train Loss: 0.5946 | Train Acc: 0.798\n",
      "  Val   Loss: 0.6098 | Val   Acc: 0.790\n",
      "\n",
      "Epoch 22/80\n",
      "  Train Loss: 0.5848 | Train Acc: 0.802\n",
      "  Val   Loss: 0.6010 | Val   Acc: 0.793\n",
      "\n",
      "Epoch 23/80\n",
      "  Train Loss: 0.5756 | Train Acc: 0.805\n",
      "  Val   Loss: 0.5932 | Val   Acc: 0.796\n",
      "\n",
      "Epoch 24/80\n",
      "  Train Loss: 0.5670 | Train Acc: 0.808\n",
      "  Val   Loss: 0.5853 | Val   Acc: 0.796\n",
      "\n",
      "Epoch 25/80\n",
      "  Train Loss: 0.5593 | Train Acc: 0.811\n",
      "  Val   Loss: 0.5784 | Val   Acc: 0.800\n",
      "\n",
      "Epoch 26/80\n",
      "  Train Loss: 0.5517 | Train Acc: 0.813\n",
      "  Val   Loss: 0.5713 | Val   Acc: 0.803\n",
      "\n",
      "Epoch 27/80\n",
      "  Train Loss: 0.5450 | Train Acc: 0.816\n",
      "  Val   Loss: 0.5649 | Val   Acc: 0.805\n",
      "\n",
      "Epoch 28/80\n",
      "  Train Loss: 0.5386 | Train Acc: 0.818\n",
      "  Val   Loss: 0.5592 | Val   Acc: 0.806\n",
      "\n",
      "Epoch 29/80\n",
      "  Train Loss: 0.5328 | Train Acc: 0.820\n",
      "  Val   Loss: 0.5542 | Val   Acc: 0.808\n",
      "\n",
      "Epoch 30/80\n",
      "  Train Loss: 0.5270 | Train Acc: 0.821\n",
      "  Val   Loss: 0.5490 | Val   Acc: 0.810\n",
      "\n",
      "Epoch 31/80\n",
      "  Train Loss: 0.5217 | Train Acc: 0.823\n",
      "  Val   Loss: 0.5447 | Val   Acc: 0.812\n",
      "\n",
      "Epoch 32/80\n",
      "  Train Loss: 0.5170 | Train Acc: 0.825\n",
      "  Val   Loss: 0.5403 | Val   Acc: 0.812\n",
      "\n",
      "Epoch 33/80\n",
      "  Train Loss: 0.5125 | Train Acc: 0.825\n",
      "  Val   Loss: 0.5356 | Val   Acc: 0.814\n",
      "\n",
      "Epoch 34/80\n",
      "  Train Loss: 0.5079 | Train Acc: 0.827\n",
      "  Val   Loss: 0.5330 | Val   Acc: 0.814\n",
      "\n",
      "Epoch 35/80\n",
      "  Train Loss: 0.5036 | Train Acc: 0.828\n",
      "  Val   Loss: 0.5291 | Val   Acc: 0.815\n",
      "\n",
      "Epoch 36/80\n",
      "  Train Loss: 0.5000 | Train Acc: 0.829\n",
      "  Val   Loss: 0.5259 | Val   Acc: 0.815\n",
      "\n",
      "Epoch 37/80\n",
      "  Train Loss: 0.4964 | Train Acc: 0.830\n",
      "  Val   Loss: 0.5225 | Val   Acc: 0.818\n",
      "\n",
      "Epoch 38/80\n",
      "  Train Loss: 0.4927 | Train Acc: 0.830\n",
      "  Val   Loss: 0.5184 | Val   Acc: 0.818\n",
      "\n",
      "Epoch 39/80\n",
      "  Train Loss: 0.4897 | Train Acc: 0.832\n",
      "  Val   Loss: 0.5166 | Val   Acc: 0.818\n",
      "\n",
      "Epoch 40/80\n",
      "  Train Loss: 0.4863 | Train Acc: 0.833\n",
      "  Val   Loss: 0.5127 | Val   Acc: 0.819\n",
      "\n",
      "Epoch 41/80\n",
      "  Train Loss: 0.4833 | Train Acc: 0.833\n",
      "  Val   Loss: 0.5118 | Val   Acc: 0.818\n",
      "\n",
      "Epoch 42/80\n",
      "  Train Loss: 0.4806 | Train Acc: 0.834\n",
      "  Val   Loss: 0.5075 | Val   Acc: 0.822\n",
      "\n",
      "Epoch 43/80\n",
      "  Train Loss: 0.4778 | Train Acc: 0.835\n",
      "  Val   Loss: 0.5043 | Val   Acc: 0.822\n",
      "\n",
      "Epoch 44/80\n",
      "  Train Loss: 0.4750 | Train Acc: 0.836\n",
      "  Val   Loss: 0.5026 | Val   Acc: 0.821\n",
      "\n",
      "Epoch 45/80\n",
      "  Train Loss: 0.4725 | Train Acc: 0.837\n",
      "  Val   Loss: 0.5009 | Val   Acc: 0.823\n",
      "\n",
      "Epoch 46/80\n",
      "  Train Loss: 0.4699 | Train Acc: 0.838\n",
      "  Val   Loss: 0.4984 | Val   Acc: 0.822\n",
      "\n",
      "Epoch 47/80\n",
      "  Train Loss: 0.4678 | Train Acc: 0.839\n",
      "  Val   Loss: 0.4975 | Val   Acc: 0.824\n",
      "\n",
      "Epoch 48/80\n",
      "  Train Loss: 0.4653 | Train Acc: 0.839\n",
      "  Val   Loss: 0.4952 | Val   Acc: 0.824\n",
      "\n",
      "Epoch 49/80\n",
      "  Train Loss: 0.4634 | Train Acc: 0.840\n",
      "  Val   Loss: 0.4922 | Val   Acc: 0.825\n",
      "\n",
      "Epoch 50/80\n",
      "  Train Loss: 0.4612 | Train Acc: 0.841\n",
      "  Val   Loss: 0.4912 | Val   Acc: 0.826\n",
      "\n",
      "Epoch 51/80\n",
      "  Train Loss: 0.4593 | Train Acc: 0.841\n",
      "  Val   Loss: 0.4889 | Val   Acc: 0.826\n",
      "\n",
      "Epoch 52/80\n",
      "  Train Loss: 0.4572 | Train Acc: 0.842\n",
      "  Val   Loss: 0.4877 | Val   Acc: 0.826\n",
      "\n",
      "Epoch 53/80\n",
      "  Train Loss: 0.4552 | Train Acc: 0.842\n",
      "  Val   Loss: 0.4852 | Val   Acc: 0.827\n",
      "\n",
      "Epoch 54/80\n",
      "  Train Loss: 0.4534 | Train Acc: 0.843\n",
      "  Val   Loss: 0.4842 | Val   Acc: 0.828\n",
      "\n",
      "Epoch 55/80\n",
      "  Train Loss: 0.4515 | Train Acc: 0.844\n",
      "  Val   Loss: 0.4832 | Val   Acc: 0.828\n",
      "\n",
      "Epoch 56/80\n",
      "  Train Loss: 0.4501 | Train Acc: 0.844\n",
      "  Val   Loss: 0.4807 | Val   Acc: 0.829\n",
      "\n",
      "Epoch 57/80\n",
      "  Train Loss: 0.4481 | Train Acc: 0.844\n",
      "  Val   Loss: 0.4791 | Val   Acc: 0.829\n",
      "\n",
      "Epoch 58/80\n",
      "  Train Loss: 0.4463 | Train Acc: 0.845\n",
      "  Val   Loss: 0.4773 | Val   Acc: 0.829\n",
      "\n",
      "Epoch 59/80\n",
      "  Train Loss: 0.4448 | Train Acc: 0.846\n",
      "  Val   Loss: 0.4765 | Val   Acc: 0.831\n",
      "\n",
      "Epoch 60/80\n",
      "  Train Loss: 0.4434 | Train Acc: 0.847\n",
      "  Val   Loss: 0.4761 | Val   Acc: 0.831\n",
      "\n",
      "Epoch 61/80\n",
      "  Train Loss: 0.4419 | Train Acc: 0.847\n",
      "  Val   Loss: 0.4727 | Val   Acc: 0.833\n",
      "\n",
      "Epoch 62/80\n",
      "  Train Loss: 0.4403 | Train Acc: 0.848\n",
      "  Val   Loss: 0.4721 | Val   Acc: 0.834\n",
      "\n",
      "Epoch 63/80\n",
      "  Train Loss: 0.4385 | Train Acc: 0.848\n",
      "  Val   Loss: 0.4718 | Val   Acc: 0.833\n",
      "\n",
      "Epoch 64/80\n",
      "  Train Loss: 0.4373 | Train Acc: 0.848\n",
      "  Val   Loss: 0.4694 | Val   Acc: 0.833\n",
      "\n",
      "Epoch 65/80\n",
      "  Train Loss: 0.4361 | Train Acc: 0.849\n",
      "  Val   Loss: 0.4684 | Val   Acc: 0.834\n",
      "\n",
      "Epoch 66/80\n",
      "  Train Loss: 0.4347 | Train Acc: 0.850\n",
      "  Val   Loss: 0.4669 | Val   Acc: 0.835\n",
      "\n",
      "Epoch 67/80\n",
      "  Train Loss: 0.4331 | Train Acc: 0.850\n",
      "  Val   Loss: 0.4654 | Val   Acc: 0.836\n",
      "\n",
      "Epoch 68/80\n",
      "  Train Loss: 0.4320 | Train Acc: 0.850\n",
      "  Val   Loss: 0.4655 | Val   Acc: 0.835\n",
      "\n",
      "Epoch 69/80\n",
      "  Train Loss: 0.4304 | Train Acc: 0.851\n",
      "  Val   Loss: 0.4627 | Val   Acc: 0.836\n",
      "\n",
      "Epoch 70/80\n",
      "  Train Loss: 0.4291 | Train Acc: 0.852\n",
      "  Val   Loss: 0.4617 | Val   Acc: 0.836\n",
      "\n",
      "Epoch 71/80\n",
      "  Train Loss: 0.4279 | Train Acc: 0.852\n",
      "  Val   Loss: 0.4617 | Val   Acc: 0.837\n",
      "\n",
      "Epoch 72/80\n",
      "  Train Loss: 0.4265 | Train Acc: 0.853\n",
      "  Val   Loss: 0.4601 | Val   Acc: 0.839\n",
      "\n",
      "Epoch 73/80\n",
      "  Train Loss: 0.4256 | Train Acc: 0.853\n",
      "  Val   Loss: 0.4598 | Val   Acc: 0.837\n",
      "\n",
      "Epoch 74/80\n",
      "  Train Loss: 0.4240 | Train Acc: 0.853\n",
      "  Val   Loss: 0.4587 | Val   Acc: 0.840\n",
      "\n",
      "Epoch 75/80\n",
      "  Train Loss: 0.4228 | Train Acc: 0.854\n",
      "  Val   Loss: 0.4573 | Val   Acc: 0.839\n",
      "\n",
      "Epoch 76/80\n",
      "  Train Loss: 0.4219 | Train Acc: 0.854\n",
      "  Val   Loss: 0.4563 | Val   Acc: 0.839\n",
      "\n",
      "Epoch 77/80\n",
      "  Train Loss: 0.4206 | Train Acc: 0.855\n",
      "  Val   Loss: 0.4545 | Val   Acc: 0.840\n",
      "\n",
      "Epoch 78/80\n",
      "  Train Loss: 0.4197 | Train Acc: 0.855\n",
      "  Val   Loss: 0.4538 | Val   Acc: 0.840\n",
      "\n",
      "Epoch 79/80\n",
      "  Train Loss: 0.4184 | Train Acc: 0.855\n",
      "  Val   Loss: 0.4556 | Val   Acc: 0.838\n",
      "\n",
      "Epoch 80/80\n",
      "  Train Loss: 0.4171 | Train Acc: 0.855\n",
      "  Val   Loss: 0.4516 | Val   Acc: 0.841\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "test = ModelPipeline()\n",
    "test.run(epochs=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93418d8a-8882-4220-a339-d1aed2185f29",
   "metadata": {},
   "source": [
    "### Explainations about the code\n",
    "\n",
    "As we can see, this class `ModelPipeline`reuse the code from the Quickstart but with more modularity and by integrating  TensorBoard.\n",
    "\n",
    "Here are the steps to use TensorBoard:\n",
    "\n",
    "1. Install it with pip: `pip install tensorboard`\n",
    "2. Import the `SummaryWriter`class : `from torch.utils.tensorboard import SummaryWriter`\n",
    "3. Create the writer object. You can specify the folder in which the run logs will be save `writer = SummaryWriter(log_dir=f\"runs/{run_name}\")`\n",
    "4. Add the scalar value of the wanted metrics with th `add_scalar(\"name_of_the_metrics\", x_metric,y_metric)`\n",
    "5. Close the writer at the end of the training.\n",
    "\n",
    "In jupyter-notebook, before executing our model pipeline, we must run `%load_ext tensorboard`to load the tensorboard and `%tensorboard --logdir runs`to indicate in which folder are the metrics we want to display."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb0937-2a5f-403d-b521-11eb44b3d396",
   "metadata": {},
   "source": [
    "## Compare different loss functions and optimizer\n",
    "\n",
    "As we specified with the parameter `--logdir` from tensorboard, we can load a folder with multiple the performances of different models configurations. For instance, the loss function or the optimizer can vary. \n",
    "\n",
    "To do so, we need to make our pipeline class more modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "512050a2-654c-4f93-bbc2-bd165eea9bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "import inspect\n",
    "from datetime import datetime\n",
    "\n",
    "class NewModelPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        dataset_fn,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        loss_fn: nn.Module,\n",
    "        batch_size: int = 64,\n",
    "        transform=None,\n",
    "        log_root: str = \"runs\",\n",
    "    ):\n",
    "        # Dataset configuration\n",
    "        transform = transform or transforms.ToTensor()\n",
    "        self.train_data = dataset_fn(root=\"data\", train=True, download=True, transform=transform)\n",
    "        self.test_data = dataset_fn(root=\"data\", train=False, download=True, transform=transform)\n",
    "        self.train_loader = DataLoader(self.train_data, batch_size=batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(self.test_data, batch_size=batch_size)\n",
    "\n",
    "        # Device setup\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using {self.device} device\")\n",
    "\n",
    "        # Model, optimizer, and loss\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        # Auto naming for TensorBoard log\n",
    "        dataset_name = dataset_fn.__name__\n",
    "        optimizer_name = self.optimizer.__class__.__name__\n",
    "        loss_name = self.loss_fn.__class__.__name__\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        log_name = f\"{dataset_name}_{optimizer_name}_{loss_name}_{timestamp}\"\n",
    "        self.writer = SummaryWriter(log_dir=f\"{log_root}/{log_name}\")\n",
    "\n",
    "    # Training step\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss, correct = 0, 0\n",
    "        for X, y in self.train_loader:\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        accuracy = correct / len(self.train_loader.dataset)\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    # Evaluation step\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        total_loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.test_loader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self.model(X)\n",
    "                total_loss += self.loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.test_loader)\n",
    "        accuracy = correct / len(self.test_loader.dataset)\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    # Full training loop\n",
    "    def fit(self, epochs=10, interactive=False):\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "            train_loss, train_acc = self.train_one_epoch()\n",
    "            val_loss, val_acc = self.evaluate()\n",
    "\n",
    "            print(f\"Train | loss: {train_loss:.4f}, acc: {train_acc:.3f}\")\n",
    "            print(f\"Val   | loss: {val_loss:.4f}, acc: {val_acc:.3f}\")\n",
    "\n",
    "            self.writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "            self.writer.add_scalar(\"Loss/validation\", val_loss, epoch)\n",
    "            self.writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "            self.writer.add_scalar(\"Accuracy/validation\", val_acc, epoch)\n",
    "\n",
    "            if interactive:\n",
    "                cont = input(\"Continue to next epoch? (y/n): \")\n",
    "                if cont.lower() != \"y\":\n",
    "                    print(\"Training stopped by user.\")\n",
    "                    break\n",
    "\n",
    "        self.writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b738a-5d76-4a45-a974-f9b02b8c695c",
   "metadata": {},
   "source": [
    "This class now allows us to choose which **dataset**, **optimizer** and **loss function** to use and the name of the logs will be defined accordingly.\n",
    "We can also choose the **model** class to use and the **batch size**.\n",
    "\n",
    "I also added the possibility to use an **interactive mode** and some parameters have default values.\n",
    "Below is an instance of our pipeline with the **Adam** optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96f82ef0-3a87-49b1-9516-591c3dca8187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 243876), started 16:26:24 ago. (Use '!kill 243876' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-42067f2c6b1e2c87\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-42067f2c6b1e2c87\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "Train | loss: 0.4852, acc: 0.824\n",
      "Val   | loss: 0.4082, acc: 0.851\n",
      "\n",
      "Epoch 2/50\n",
      "Train | loss: 0.3568, acc: 0.869\n",
      "Val   | loss: 0.3702, acc: 0.862\n",
      "\n",
      "Epoch 3/50\n",
      "Train | loss: 0.3202, acc: 0.881\n",
      "Val   | loss: 0.3515, acc: 0.869\n",
      "\n",
      "Epoch 4/50\n",
      "Train | loss: 0.2988, acc: 0.888\n",
      "Val   | loss: 0.3748, acc: 0.866\n",
      "\n",
      "Epoch 5/50\n",
      "Train | loss: 0.2769, acc: 0.896\n",
      "Val   | loss: 0.3488, acc: 0.876\n",
      "\n",
      "Epoch 6/50\n",
      "Train | loss: 0.2643, acc: 0.901\n",
      "Val   | loss: 0.3342, acc: 0.884\n",
      "\n",
      "Epoch 7/50\n",
      "Train | loss: 0.2488, acc: 0.906\n",
      "Val   | loss: 0.3271, acc: 0.878\n",
      "\n",
      "Epoch 8/50\n",
      "Train | loss: 0.2372, acc: 0.910\n",
      "Val   | loss: 0.3347, acc: 0.881\n",
      "\n",
      "Epoch 9/50\n",
      "Train | loss: 0.2264, acc: 0.913\n",
      "Val   | loss: 0.3536, acc: 0.882\n",
      "\n",
      "Epoch 10/50\n",
      "Train | loss: 0.2153, acc: 0.918\n",
      "Val   | loss: 0.3160, acc: 0.890\n",
      "\n",
      "Epoch 11/50\n",
      "Train | loss: 0.2045, acc: 0.922\n",
      "Val   | loss: 0.3320, acc: 0.891\n",
      "\n",
      "Epoch 12/50\n",
      "Train | loss: 0.1968, acc: 0.924\n",
      "Val   | loss: 0.3531, acc: 0.882\n",
      "\n",
      "Epoch 13/50\n",
      "Train | loss: 0.1884, acc: 0.928\n",
      "Val   | loss: 0.3415, acc: 0.886\n",
      "\n",
      "Epoch 14/50\n",
      "Train | loss: 0.1824, acc: 0.931\n",
      "Val   | loss: 0.3289, acc: 0.894\n",
      "\n",
      "Epoch 15/50\n",
      "Train | loss: 0.1735, acc: 0.934\n",
      "Val   | loss: 0.3267, acc: 0.894\n",
      "\n",
      "Epoch 16/50\n",
      "Train | loss: 0.1669, acc: 0.935\n",
      "Val   | loss: 0.3590, acc: 0.894\n",
      "\n",
      "Epoch 17/50\n",
      "Train | loss: 0.1619, acc: 0.937\n",
      "Val   | loss: 0.3565, acc: 0.895\n",
      "\n",
      "Epoch 18/50\n",
      "Train | loss: 0.1548, acc: 0.940\n",
      "Val   | loss: 0.3552, acc: 0.890\n",
      "\n",
      "Epoch 19/50\n",
      "Train | loss: 0.1489, acc: 0.942\n",
      "Val   | loss: 0.3805, acc: 0.894\n",
      "\n",
      "Epoch 20/50\n",
      "Train | loss: 0.1438, acc: 0.943\n",
      "Val   | loss: 0.3777, acc: 0.894\n",
      "\n",
      "Epoch 21/50\n",
      "Train | loss: 0.1371, acc: 0.946\n",
      "Val   | loss: 0.4464, acc: 0.883\n",
      "\n",
      "Epoch 22/50\n",
      "Train | loss: 0.1327, acc: 0.948\n",
      "Val   | loss: 0.4235, acc: 0.892\n",
      "\n",
      "Epoch 23/50\n",
      "Train | loss: 0.1273, acc: 0.950\n",
      "Val   | loss: 0.4922, acc: 0.889\n",
      "\n",
      "Epoch 24/50\n",
      "Train | loss: 0.1232, acc: 0.952\n",
      "Val   | loss: 0.4130, acc: 0.890\n",
      "\n",
      "Epoch 25/50\n",
      "Train | loss: 0.1184, acc: 0.953\n",
      "Val   | loss: 0.4088, acc: 0.897\n",
      "\n",
      "Epoch 26/50\n",
      "Train | loss: 0.1157, acc: 0.955\n",
      "Val   | loss: 0.4443, acc: 0.895\n",
      "\n",
      "Epoch 27/50\n",
      "Train | loss: 0.1117, acc: 0.956\n",
      "Val   | loss: 0.4957, acc: 0.895\n",
      "\n",
      "Epoch 28/50\n",
      "Train | loss: 0.1083, acc: 0.958\n",
      "Val   | loss: 0.4764, acc: 0.891\n",
      "\n",
      "Epoch 29/50\n",
      "Train | loss: 0.1077, acc: 0.958\n",
      "Val   | loss: 0.5197, acc: 0.885\n",
      "\n",
      "Epoch 30/50\n",
      "Train | loss: 0.1005, acc: 0.961\n",
      "Val   | loss: 0.5267, acc: 0.885\n",
      "\n",
      "Epoch 31/50\n",
      "Train | loss: 0.0964, acc: 0.962\n",
      "Val   | loss: 0.5092, acc: 0.887\n",
      "\n",
      "Epoch 32/50\n",
      "Train | loss: 0.0961, acc: 0.963\n",
      "Val   | loss: 0.5015, acc: 0.892\n",
      "\n",
      "Epoch 33/50\n",
      "Train | loss: 0.0918, acc: 0.964\n",
      "Val   | loss: 0.5686, acc: 0.890\n",
      "\n",
      "Epoch 34/50\n",
      "Train | loss: 0.0907, acc: 0.965\n",
      "Val   | loss: 0.5441, acc: 0.893\n",
      "\n",
      "Epoch 35/50\n",
      "Train | loss: 0.0855, acc: 0.968\n",
      "Val   | loss: 0.5549, acc: 0.891\n",
      "\n",
      "Epoch 36/50\n",
      "Train | loss: 0.0822, acc: 0.968\n",
      "Val   | loss: 0.5702, acc: 0.893\n",
      "\n",
      "Epoch 37/50\n",
      "Train | loss: 0.0867, acc: 0.967\n",
      "Val   | loss: 0.5502, acc: 0.894\n",
      "\n",
      "Epoch 38/50\n",
      "Train | loss: 0.0756, acc: 0.971\n",
      "Val   | loss: 0.5634, acc: 0.893\n",
      "\n",
      "Epoch 39/50\n",
      "Train | loss: 0.0766, acc: 0.971\n",
      "Val   | loss: 0.6597, acc: 0.880\n",
      "\n",
      "Epoch 40/50\n",
      "Train | loss: 0.0833, acc: 0.969\n",
      "Val   | loss: 0.5711, acc: 0.893\n",
      "\n",
      "Epoch 41/50\n",
      "Train | loss: 0.0676, acc: 0.973\n",
      "Val   | loss: 0.6389, acc: 0.896\n",
      "\n",
      "Epoch 42/50\n",
      "Train | loss: 0.0696, acc: 0.973\n",
      "Val   | loss: 0.6224, acc: 0.898\n",
      "\n",
      "Epoch 43/50\n",
      "Train | loss: 0.0723, acc: 0.972\n",
      "Val   | loss: 0.6030, acc: 0.888\n",
      "\n",
      "Epoch 44/50\n",
      "Train | loss: 0.0677, acc: 0.974\n",
      "Val   | loss: 0.6758, acc: 0.895\n",
      "\n",
      "Epoch 45/50\n",
      "Train | loss: 0.0690, acc: 0.974\n",
      "Val   | loss: 0.6572, acc: 0.897\n",
      "\n",
      "Epoch 46/50\n",
      "Train | loss: 0.0671, acc: 0.974\n",
      "Val   | loss: 0.6375, acc: 0.890\n",
      "\n",
      "Epoch 47/50\n",
      "Train | loss: 0.0636, acc: 0.976\n",
      "Val   | loss: 0.6730, acc: 0.890\n",
      "\n",
      "Epoch 48/50\n",
      "Train | loss: 0.0620, acc: 0.977\n",
      "Val   | loss: 0.7000, acc: 0.898\n",
      "\n",
      "Epoch 49/50\n",
      "Train | loss: 0.0613, acc: 0.977\n",
      "Val   | loss: 0.7178, acc: 0.895\n",
      "\n",
      "Epoch 50/50\n",
      "Train | loss: 0.0595, acc: 0.977\n",
      "Val   | loss: 0.6610, acc: 0.892\n"
     ]
    }
   ],
   "source": [
    "# Instantiate components outside (for full flexibility)\n",
    "model = NeuralNetwork()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) \n",
    "# Create pipeline\n",
    "pipeline = NewModelPipeline(\n",
    "    model=model,\n",
    "    dataset_fn=datasets.FashionMNIST,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "# Initialize the tensorboard \n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs\n",
    "\n",
    "# Run training (interactive=False for normal training)\n",
    "pipeline.fit(epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05063a65-3608-4005-98a2-faf26f090572",
   "metadata": {},
   "source": [
    "We can now easly compare the two optimizers with *TensorBoard* now!\n",
    "\n",
    "While it seems that for the **SGD** with a learning rate of 0.001 performanes cap around **84%** accuracy and there is no overfitting even after 80 epochs, using **Adam** with also a learning rate of 0.001 make our model reach more than **90%** accuracy but overfit after only 10 epochs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5262737-2d5b-4f0c-9beb-8473941f3aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
